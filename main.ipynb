{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f3f99-50df-469e-a2c0-71a0324f764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44fdd3fd-0836-4dd6-9fef-0c9d65429c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8153e0f-4f19-4494-b047-f3f12f37e338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Demensdeum\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n",
      "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]\n",
      "['C:\\\\Users\\\\Demensdeum\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310', 'C:\\\\Users\\\\Demensdeum\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "import site\n",
    "print(site.getsitepackages())\n",
    "import os\n",
    "print(os.environ.get('PATH'))\n",
    "print(os.environ.get('CUDA_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eec375ef-5fc7-4404-80a0-13a0318268f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 85352 messages with specific columns to 'messages.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def flatten_chat_messages_with_specific_columns(input_filename=\"result.json\", output_filename=\"messages.json\"):\n",
    "    try:\n",
    "        with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_filename}' not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{input_filename}'.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading '{input_filename}': {e}\")\n",
    "        return\n",
    "\n",
    "    selected_messages = []\n",
    "    desired_columns = [\"from\", \"text\", \"date\"]\n",
    "\n",
    "    if 'chats' in data and isinstance(data['chats'], dict) and \\\n",
    "       'list' in data['chats'] and isinstance(data['chats']['list'], list):\n",
    "        for chat_entry in data['chats']['list']:\n",
    "            if not 'name' in chat_entry:\n",
    "                continue\n",
    "                \n",
    "            chat_id = chat_entry['name']\n",
    "            \n",
    "            if chat_id == None:\n",
    "                chat_id = \"Unknown\"\n",
    "                \n",
    "            if 'messages' in chat_entry and isinstance(chat_entry['messages'], list):\n",
    "                for message in chat_entry['messages']:\n",
    "                    filtered_message = {}\n",
    "                    for col in desired_columns:\n",
    "                        if col in message:\n",
    "                            filtered_message[col] = message[col]\n",
    "\n",
    "                    if filtered_message and len(filtered_message[\"text\"]) > 0:\n",
    "                        filtered_message[\"chat_id\"] = chat_id\n",
    "                        selected_messages.append(filtered_message)\n",
    "    else:\n",
    "        print(\"Warning: Expected 'chats.list' structure not found or is not in the correct format in your input JSON. No messages will be extracted.\")\n",
    "\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(selected_messages, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Successfully extracted {len(selected_messages)} messages with specific columns to '{output_filename}'.\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to output file '{output_filename}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while writing '{output_filename}': {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    flatten_chat_messages_with_specific_columns(\"result.json\", \"messages.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf3261b2-c7f6-4a55-904f-9d5d32a9f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('messages.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9baf7e6-2a87-457b-b273-9c4bda8df32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             from  \\\n",
      "6270  Ilia Prokhorov (DemensDeum)   \n",
      "6271  Ilia Prokhorov (DemensDeum)   \n",
      "6272  Ilia Prokhorov (DemensDeum)   \n",
      "6273               Серега Федоров   \n",
      "6274               Серега Федоров   \n",
      "...                           ...   \n",
      "6269                Игорь Беларус   \n",
      "6194  Ilia Prokhorov (DemensDeum)   \n",
      "6195                Mariya Bogach   \n",
      "6196                Mariya Bogach   \n",
      "6197  Ilia Prokhorov (DemensDeum)   \n",
      "\n",
      "                                                   text                date  \\\n",
      "6270                                              Йопта 2016-12-28 01:56:43   \n",
      "6271                                 Так инсту заведешь 2016-12-28 01:56:48   \n",
      "6272                     Че за картонка у тебя в руках? 2017-04-14 20:39:26   \n",
      "6273              Вот ток ты сказал, а его уже выпилили 2017-04-18 20:03:33   \n",
      "6274  Steve Stephens Dead: 5 Fast Facts You Need to ... 2017-04-18 20:03:40   \n",
      "...                                                 ...                 ...   \n",
      "6269                                             Чикаго 2025-07-19 16:38:17   \n",
      "6194               Настолько типичная Европа что Европа 2025-07-19 17:03:26   \n",
      "6195                                            Я ржала 2025-07-19 17:13:51   \n",
      "6196  Она пишет, что в Геленджике горки и то пизже, ... 2025-07-19 17:14:08   \n",
      "6197         бля да в Европе хуевей местами чем в Батум 2025-07-19 17:26:18   \n",
      "\n",
      "     chat_id  \n",
      "6270  Серега  \n",
      "6271  Серега  \n",
      "6272  Серега  \n",
      "6273  Серега  \n",
      "6274  Серега  \n",
      "...      ...  \n",
      "6269   Игорь  \n",
      "6194  Mariya  \n",
      "6195  Mariya  \n",
      "6196  Mariya  \n",
      "6197  Mariya  \n",
      "\n",
      "[85351 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#target_chat_id_prefix = \"Bars\"\n",
    "#filtered_df = df[df['chat_id'].astype(str).str.startswith(target_chat_id_prefix, na=False)]\n",
    "#sorted_df = filtered_df.sort_values(by='date')\n",
    "#print(f\"\\nMessages for chat_id: '{target_chat_id}' (sorted by date):\")\n",
    "\n",
    "sorted_df = df.sort_values(by='date')\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "403e8128-548b-44a7-9427-6a404474e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chat: #самарскийлонгбординг for finetuning data...\n",
      "\n",
      "Processing chat: .:NeCrOmAnCeR:. for finetuning data...\n",
      "\n",
      "Processing chat: 0JLQuCDQotGP0L0= for finetuning data...\n",
      "\n",
      "Processing chat: Aleksandr for finetuning data...\n",
      "\n",
      "Processing chat: Alex for finetuning data...\n",
      "\n",
      "Processing chat: Alexey for finetuning data...\n",
      "\n",
      "Processing chat: AllTrust.me for finetuning data...\n",
      "\n",
      "Processing chat: Andrew Smirnov for finetuning data...\n",
      "\n",
      "Processing chat: AntiSwap for finetuning data...\n",
      "\n",
      "Processing chat: Anton for finetuning data...\n",
      "\n",
      "Processing chat: Bars for finetuning data...\n",
      "\n",
      "Processing chat: BonBon for finetuning data...\n",
      "\n",
      "Processing chat: CPT for finetuning data...\n",
      "\n",
      "Processing chat: Capsule Hotel Tbilisi for finetuning data...\n",
      "\n",
      "Processing chat: Cheque for finetuning data...\n",
      "\n",
      "Processing chat: Credo Bank for finetuning data...\n",
      "\n",
      "Processing chat: Daniil Polyakov for finetuning data...\n",
      "\n",
      "Processing chat: Denis for finetuning data...\n",
      "\n",
      "Processing chat: Diwixis for finetuning data...\n",
      "\n",
      "Processing chat: Dаша for finetuning data...\n",
      "\n",
      "Processing chat: EDB for finetuning data...\n",
      "\n",
      "Processing chat: Ed for finetuning data...\n",
      "\n",
      "Processing chat: Eugene Женек for finetuning data...\n",
      "\n",
      "Processing chat: Eugenii for finetuning data...\n",
      "\n",
      "Processing chat: Evgeny for finetuning data...\n",
      "\n",
      "Processing chat: FFCB® banker for finetuning data...\n",
      "\n",
      "Processing chat: Giorgi ИП Помогатор for finetuning data...\n",
      "\n",
      "Processing chat: Gregory for finetuning data...\n",
      "\n",
      "Processing chat: Helenka Demidova for finetuning data...\n",
      "\n",
      "Processing chat: Husky🇬🇪 for finetuning data...\n",
      "\n",
      "Processing chat: Igor for finetuning data...\n",
      "\n",
      "Processing chat: Ivan for finetuning data...\n",
      "\n",
      "Processing chat: Just Dude for finetuning data...\n",
      "\n",
      "Processing chat: Kalashnikova for finetuning data...\n",
      "\n",
      "Processing chat: Kermen for finetuning data...\n",
      "\n",
      "Processing chat: Kirill for finetuning data...\n",
      "\n",
      "Processing chat: Kris for finetuning data...\n",
      "\n",
      "Processing chat: Kseniya for finetuning data...\n",
      "\n",
      "Processing chat: Lake Bus Dev for finetuning data...\n",
      "\n",
      "Processing chat: Mantas for finetuning data...\n",
      "\n",
      "Processing chat: Mariya for finetuning data...\n",
      "\n",
      "Processing chat: Max for finetuning data...\n",
      "\n",
      "Processing chat: Mikhail for finetuning data...\n",
      "\n",
      "Processing chat: Nikita for finetuning data...\n",
      "\n",
      "Processing chat: Nikus for finetuning data...\n",
      "\n",
      "Processing chat: Ravestag Support for finetuning data...\n",
      "\n",
      "Processing chat: STANISLAV for finetuning data...\n",
      "\n",
      "Processing chat: Sauceory for finetuning data...\n",
      "\n",
      "Processing chat: Sberbank for finetuning data...\n",
      "\n",
      "Processing chat: Sergei Pauli for finetuning data...\n",
      "\n",
      "Processing chat: Sher1F Crypto for finetuning data...\n",
      "\n",
      "Processing chat: Stanislav for finetuning data...\n",
      "\n",
      "Processing chat: Stas for finetuning data...\n",
      "\n",
      "Processing chat: Surfskate_SMR for finetuning data...\n",
      "\n",
      "Processing chat: Swift for finetuning data...\n",
      "\n",
      "Processing chat: Tamara for finetuning data...\n",
      "\n",
      "Processing chat: Tatiana Подруга Маши for finetuning data...\n",
      "\n",
      "Processing chat: Telegram for finetuning data...\n",
      "\n",
      "Processing chat: _Awasaky_ for finetuning data...\n",
      "\n",
      "Processing chat: allpeg for finetuning data...\n",
      "\n",
      "Processing chat: chaga for finetuning data...\n",
      "\n",
      "Processing chat: mftidatascience for finetuning data...\n",
      "\n",
      "Processing chat: polina for finetuning data...\n",
      "\n",
      "Processing chat: tupik batumi hotel for finetuning data...\n",
      "\n",
      "Processing chat: valeriya Design for finetuning data...\n",
      "\n",
      "Processing chat: vanya_bffn for finetuning data...\n",
      "\n",
      "Processing chat: ­Yury for finetuning data...\n",
      "\n",
      "Processing chat: Ādam for finetuning data...\n",
      "\n",
      "Processing chat: Аwww Артур Хакатон for finetuning data...\n",
      "\n",
      "Processing chat: Александра for finetuning data...\n",
      "\n",
      "Processing chat: Алексей for finetuning data...\n",
      "\n",
      "Processing chat: Анастасия for finetuning data...\n",
      "\n",
      "Processing chat: Анвар for finetuning data...\n",
      "\n",
      "Processing chat: Андрей for finetuning data...\n",
      "\n",
      "Processing chat: Анна for finetuning data...\n",
      "\n",
      "Processing chat: Артем for finetuning data...\n",
      "\n",
      "Processing chat: Вероника for finetuning data...\n",
      "\n",
      "Processing chat: Ви for finetuning data...\n",
      "\n",
      "Processing chat: Владимир for finetuning data...\n",
      "\n",
      "Processing chat: Вовчик Колесников for finetuning data...\n",
      "\n",
      "Processing chat: ГЕЙМДЖЕМ ОТ «НАЧНИ ИГРУ» 24-27 НОЯБРЯ 2023 г. for finetuning data...\n",
      "\n",
      "Processing chat: Георгий for finetuning data...\n",
      "\n",
      "Processing chat: Денис for finetuning data...\n",
      "\n",
      "Processing chat: Дмитрий for finetuning data...\n",
      "\n",
      "Processing chat: Егор for finetuning data...\n",
      "\n",
      "Processing chat: Елена for finetuning data...\n",
      "\n",
      "Processing chat: ИП Грузия Анлександра Менеджер for finetuning data...\n",
      "\n",
      "Processing chat: Игорь for finetuning data...\n",
      "\n",
      "Processing chat: Игра в кальмара в Пятничном баре for finetuning data...\n",
      "\n",
      "Processing chat: Ира for finetuning data...\n",
      "\n",
      "Processing chat: Калинин for finetuning data...\n",
      "\n",
      "Processing chat: Катя for finetuning data...\n",
      "\n",
      "Processing chat: Катя Чернус Анталия for finetuning data...\n",
      "\n",
      "Processing chat: Ксения for finetuning data...\n",
      "\n",
      "Processing chat: Лунарная for finetuning data...\n",
      "\n",
      "Processing chat: Маша for finetuning data...\n",
      "\n",
      "Processing chat: Наталья for finetuning data...\n",
      "\n",
      "Processing chat: Не for finetuning data...\n",
      "\n",
      "Processing chat: Нелли (Лейка) for finetuning data...\n",
      "\n",
      "Processing chat: Оксана for finetuning data...\n",
      "\n",
      "Processing chat: Островская Chat for finetuning data...\n",
      "\n",
      "Processing chat: Павел for finetuning data...\n",
      "\n",
      "Processing chat: Прихожук for finetuning data...\n",
      "\n",
      "Processing chat: Радмила for finetuning data...\n",
      "\n",
      "Processing chat: Саша for finetuning data...\n",
      "\n",
      "Processing chat: Саша Прохорова for finetuning data...\n",
      "\n",
      "Processing chat: Светлана for finetuning data...\n",
      "\n",
      "Processing chat: Семён for finetuning data...\n",
      "\n",
      "Processing chat: Серега for finetuning data...\n",
      "\n",
      "Processing chat: Таня for finetuning data...\n",
      "\n",
      "Processing chat: ᴘᴀᴘᴇʀʙᴀᴄᴋ ᴡʀɪᴛᴇʀ for finetuning data...\n",
      "\n",
      "Processing chat: 엑시 for finetuning data...\n",
      "\n",
      "Successfully created 19076 finetuning examples in 'ollama_finetune_dataset.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "def create_ollama_finetune_dataset(sorted_messages_df, source_id, output_finetune_file):\n",
    "    if sorted_messages_df.empty:\n",
    "        print(\"No messages provided to create finetuning dataset.\")\n",
    "        return\n",
    "\n",
    "    finetune_examples = []\n",
    "    current_instruction = None\n",
    "\n",
    "    for chat_id, chat_df in sorted_messages_df.groupby('chat_id'):\n",
    "        print(f\"\\nProcessing chat: {chat_id} for finetuning data...\")\n",
    "        current_instruction = \"\"\n",
    "\n",
    "        for index, message in chat_df.iterrows():\n",
    "            sender = message.get('from')\n",
    "            text = message.get('text')\n",
    "\n",
    "            if sender == source_id:\n",
    "                if len(current_instruction) > 0:\n",
    "                    finetune_examples.append({\n",
    "                        \"input\": current_instruction,\n",
    "                        \"output\": text\n",
    "                    })\n",
    "                    current_instruction = \"\"\n",
    "            else:\n",
    "                current_instruction += str(text)\n",
    "\n",
    "    try:\n",
    "        with open(output_finetune_file, 'w', encoding='utf-8') as f:\n",
    "            for example in finetune_examples:\n",
    "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "        print(f\"\\nSuccessfully created {len(finetune_examples)} finetuning examples in '{output_finetune_file}'.\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to output file '{output_finetune_file}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while writing '{output_finetune_file}': {e}\")\n",
    "\n",
    "create_ollama_finetune_dataset(sorted_df, \"Ilia Prokhorov (DemensDeum)\", \"ollama_finetune_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea654d79-5846-4fef-b235-b3c269c66fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13629836-ff57-4d5d-84e3-fa05b26cda1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.5: Fast Llama patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3080 Laptop GPU. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "target_model = \"unsloth/tinyllama-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = target_model,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea61cac-d5e4-430a-ac60-a138f65dc26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.5 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    \n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "083dcae5-9477-4887-9532-20d7903bba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "formatted_data = []\n",
    "\n",
    "with jsonlines.open(\"ollama_finetune_dataset.jsonl\", 'r') as reader:\n",
    "    for obj in reader:\n",
    "        output = f\"### Input: {obj['input']}\\n### Output: {json.dumps(obj['output'])}<|endoftext|>\"\n",
    "        formatted_data.append(output)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc63bc6-330b-41e8-8ce6-cfee1648a67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2653f82047444f8baa2bfbcdce36b451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/19076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 19,076 | Num Epochs = 1 | Total steps = 4,769\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 12,615,680 of 1,112,664,064 (1.13% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1945' max='4769' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1945/4769 1:04:21 < 1:33:32, 0.50 it/s, Epoch 0.41/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.042200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "default_train_params = {\n",
    "    \"max_seq_length\" : max_seq_length,\n",
    "    \"dataset_num_proc\" : 2,\n",
    "    \"per_device_train_batch_size\" : 2,\n",
    "    \"gradient_accumulation_steps\" : 4,\n",
    "    \"warmup_steps\" : 10,\n",
    "    \"num_train_epochs\" : 3,\n",
    "    \"learning_rate\" : 2e-4,\n",
    "    \"fp16\" : not torch.cuda.is_bf16_supported(),\n",
    "    \"bf16\" : torch.cuda.is_bf16_supported(),\n",
    "    \"logging_steps\" : 25,  \n",
    "    \"weight_decay\" : 0.01,\n",
    "    \"seed\" : 3407,\n",
    "    \"save_total_limit\" : 2,\n",
    "    \"dataloader_pin_memory\" : False\n",
    "}\n",
    "\n",
    "faster_train_params = {\n",
    "    \"max_seq_length\" : 512,\n",
    "    \"dataset_num_proc\" : 4,\n",
    "    \"per_device_train_batch_size\" : 4,\n",
    "    \"gradient_accumulation_steps\" : 1,\n",
    "    \"warmup_steps\" : 0,\n",
    "    \"num_train_epochs\" : 1,\n",
    "    \"learning_rate\" : 5e-4,\n",
    "    \"fp16\" : not torch.cuda.is_bf16_supported(),\n",
    "    \"bf16\" : torch.cuda.is_bf16_supported(),\n",
    "    \"logging_steps\" : 100,  \n",
    "    \"weight_decay\" : 0.0,\n",
    "    \"seed\" : 3407,\n",
    "    \"save_total_limit\" : 0,\n",
    "    \"dataloader_pin_memory\" : True\n",
    "}\n",
    "\n",
    "train_params = faster_train_params\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=train_params[\"max_seq_length\"],\n",
    "    dataset_num_proc=train_params[\"dataset_num_proc\"],\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=train_params[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps=train_params[\"gradient_accumulation_steps\"],  # Effective batch size = 8\n",
    "        warmup_steps=train_params[\"warmup_steps\"],\n",
    "        num_train_epochs=train_params[\"num_train_epochs\"],\n",
    "        learning_rate=train_params[\"learning_rate\"],\n",
    "        fp16=train_params[\"fp16\"],\n",
    "        bf16=train_params[\"bf16\"],\n",
    "        logging_steps=train_params[\"logging_steps\"],\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=train_params[\"weight_decay\"],\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=train_params[\"seed\"],\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=train_params[\"save_total_limit\"],\n",
    "        dataloader_pin_memory=train_params[\"dataloader_pin_memory\"],\n",
    "        report_to=\"none\", # Disable Weights & Biases logging\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5d454-258d-4636-a02d-98e48896d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9b065-e4d4-4334-b20a-50b7c1864df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
