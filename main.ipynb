{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f3f99-50df-469e-a2c0-71a0324f764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44fdd3fd-0836-4dd6-9fef-0c9d65429c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8153e0f-4f19-4494-b047-f3f12f37e338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Demensdeum\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n",
      "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]\n",
      "['C:\\\\Users\\\\Demensdeum\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310', 'C:\\\\Users\\\\Demensdeum\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "import site\n",
    "print(site.getsitepackages())\n",
    "import os\n",
    "print(os.environ.get('PATH'))\n",
    "print(os.environ.get('CUDA_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eec375ef-5fc7-4404-80a0-13a0318268f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 85352 messages with specific columns to 'messages.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def flatten_chat_messages_with_specific_columns(input_filename=\"result.json\", output_filename=\"messages.json\"):\n",
    "    try:\n",
    "        with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_filename}' not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{input_filename}'.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading '{input_filename}': {e}\")\n",
    "        return\n",
    "\n",
    "    selected_messages = []\n",
    "    desired_columns = [\"from\", \"text\", \"date\"]\n",
    "\n",
    "    if 'chats' in data and isinstance(data['chats'], dict) and \\\n",
    "       'list' in data['chats'] and isinstance(data['chats']['list'], list):\n",
    "        for chat_entry in data['chats']['list']:\n",
    "            if not 'name' in chat_entry:\n",
    "                continue\n",
    "                \n",
    "            chat_id = chat_entry['name']\n",
    "            \n",
    "            if chat_id == None:\n",
    "                chat_id = \"Unknown\"\n",
    "                \n",
    "            if 'messages' in chat_entry and isinstance(chat_entry['messages'], list):\n",
    "                for message in chat_entry['messages']:\n",
    "                    filtered_message = {}\n",
    "                    for col in desired_columns:\n",
    "                        if col in message:\n",
    "                            filtered_message[col] = message[col]\n",
    "\n",
    "                    if filtered_message and len(filtered_message[\"text\"]) > 0:\n",
    "                        filtered_message[\"chat_id\"] = chat_id\n",
    "                        selected_messages.append(filtered_message)\n",
    "    else:\n",
    "        print(\"Warning: Expected 'chats.list' structure not found or is not in the correct format in your input JSON. No messages will be extracted.\")\n",
    "\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(selected_messages, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Successfully extracted {len(selected_messages)} messages with specific columns to '{output_filename}'.\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to output file '{output_filename}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while writing '{output_filename}': {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    flatten_chat_messages_with_specific_columns(\"result.json\", \"messages.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf3261b2-c7f6-4a55-904f-9d5d32a9f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('messages.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9baf7e6-2a87-457b-b273-9c4bda8df32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             from  \\\n",
      "6270  Ilia Prokhorov (DemensDeum)   \n",
      "6271  Ilia Prokhorov (DemensDeum)   \n",
      "6272  Ilia Prokhorov (DemensDeum)   \n",
      "6273               Ð¡ÐµÑ€ÐµÐ³Ð° Ð¤ÐµÐ´Ð¾Ñ€Ð¾Ð²   \n",
      "6274               Ð¡ÐµÑ€ÐµÐ³Ð° Ð¤ÐµÐ´Ð¾Ñ€Ð¾Ð²   \n",
      "...                           ...   \n",
      "6269                Ð˜Ð³Ð¾Ñ€ÑŒ Ð‘ÐµÐ»Ð°Ñ€ÑƒÑ   \n",
      "6194  Ilia Prokhorov (DemensDeum)   \n",
      "6195                Mariya Bogach   \n",
      "6196                Mariya Bogach   \n",
      "6197  Ilia Prokhorov (DemensDeum)   \n",
      "\n",
      "                                                   text                date  \\\n",
      "6270                                              Ð™Ð¾Ð¿Ñ‚Ð° 2016-12-28 01:56:43   \n",
      "6271                                 Ð¢Ð°Ðº Ð¸Ð½ÑÑ‚Ñƒ Ð·Ð°Ð²ÐµÐ´ÐµÑˆÑŒ 2016-12-28 01:56:48   \n",
      "6272                     Ð§Ðµ Ð·Ð° ÐºÐ°Ñ€Ñ‚Ð¾Ð½ÐºÐ° Ñƒ Ñ‚ÐµÐ±Ñ Ð² Ñ€ÑƒÐºÐ°Ñ…? 2017-04-14 20:39:26   \n",
      "6273              Ð’Ð¾Ñ‚ Ñ‚Ð¾Ðº Ñ‚Ñ‹ ÑÐºÐ°Ð·Ð°Ð», Ð° ÐµÐ³Ð¾ ÑƒÐ¶Ðµ Ð²Ñ‹Ð¿Ð¸Ð»Ð¸Ð»Ð¸ 2017-04-18 20:03:33   \n",
      "6274  Steve Stephens Dead: 5 Fast Facts You Need to ... 2017-04-18 20:03:40   \n",
      "...                                                 ...                 ...   \n",
      "6269                                             Ð§Ð¸ÐºÐ°Ð³Ð¾ 2025-07-19 16:38:17   \n",
      "6194               ÐÐ°ÑÑ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ð¸Ð¿Ð¸Ñ‡Ð½Ð°Ñ Ð•Ð²Ñ€Ð¾Ð¿Ð° Ñ‡Ñ‚Ð¾ Ð•Ð²Ñ€Ð¾Ð¿Ð° 2025-07-19 17:03:26   \n",
      "6195                                            Ð¯ Ñ€Ð¶Ð°Ð»Ð° 2025-07-19 17:13:51   \n",
      "6196  ÐžÐ½Ð° Ð¿Ð¸ÑˆÐµÑ‚, Ñ‡Ñ‚Ð¾ Ð² Ð“ÐµÐ»ÐµÐ½Ð´Ð¶Ð¸ÐºÐµ Ð³Ð¾Ñ€ÐºÐ¸ Ð¸ Ñ‚Ð¾ Ð¿Ð¸Ð·Ð¶Ðµ, ... 2025-07-19 17:14:08   \n",
      "6197         Ð±Ð»Ñ Ð´Ð° Ð² Ð•Ð²Ñ€Ð¾Ð¿Ðµ Ñ…ÑƒÐµÐ²ÐµÐ¹ Ð¼ÐµÑÑ‚Ð°Ð¼Ð¸ Ñ‡ÐµÐ¼ Ð² Ð‘Ð°Ñ‚ÑƒÐ¼ 2025-07-19 17:26:18   \n",
      "\n",
      "     chat_id  \n",
      "6270  Ð¡ÐµÑ€ÐµÐ³Ð°  \n",
      "6271  Ð¡ÐµÑ€ÐµÐ³Ð°  \n",
      "6272  Ð¡ÐµÑ€ÐµÐ³Ð°  \n",
      "6273  Ð¡ÐµÑ€ÐµÐ³Ð°  \n",
      "6274  Ð¡ÐµÑ€ÐµÐ³Ð°  \n",
      "...      ...  \n",
      "6269   Ð˜Ð³Ð¾Ñ€ÑŒ  \n",
      "6194  Mariya  \n",
      "6195  Mariya  \n",
      "6196  Mariya  \n",
      "6197  Mariya  \n",
      "\n",
      "[85351 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#target_chat_id_prefix = \"Bars\"\n",
    "#filtered_df = df[df['chat_id'].astype(str).str.startswith(target_chat_id_prefix, na=False)]\n",
    "#sorted_df = filtered_df.sort_values(by='date')\n",
    "#print(f\"\\nMessages for chat_id: '{target_chat_id}' (sorted by date):\")\n",
    "\n",
    "sorted_df = df.sort_values(by='date')\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "403e8128-548b-44a7-9427-6a404474e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chat: #ÑÐ°Ð¼Ð°Ñ€ÑÐºÐ¸Ð¹Ð»Ð¾Ð½Ð³Ð±Ð¾Ñ€Ð´Ð¸Ð½Ð³ for finetuning data...\n",
      "\n",
      "Processing chat: .:NeCrOmAnCeR:. for finetuning data...\n",
      "\n",
      "Processing chat: 0JLQuCDQotGP0L0= for finetuning data...\n",
      "\n",
      "Processing chat: Aleksandr for finetuning data...\n",
      "\n",
      "Processing chat: Alex for finetuning data...\n",
      "\n",
      "Processing chat: Alexey for finetuning data...\n",
      "\n",
      "Processing chat: AllTrust.me for finetuning data...\n",
      "\n",
      "Processing chat: Andrew Smirnov for finetuning data...\n",
      "\n",
      "Processing chat: AntiSwap for finetuning data...\n",
      "\n",
      "Processing chat: Anton for finetuning data...\n",
      "\n",
      "Processing chat: Bars for finetuning data...\n",
      "\n",
      "Processing chat: BonBon for finetuning data...\n",
      "\n",
      "Processing chat: CPT for finetuning data...\n",
      "\n",
      "Processing chat: Capsule Hotel Tbilisi for finetuning data...\n",
      "\n",
      "Processing chat: Cheque for finetuning data...\n",
      "\n",
      "Processing chat: Credo Bank for finetuning data...\n",
      "\n",
      "Processing chat: Daniil Polyakov for finetuning data...\n",
      "\n",
      "Processing chat: Denis for finetuning data...\n",
      "\n",
      "Processing chat: Diwixis for finetuning data...\n",
      "\n",
      "Processing chat: DÐ°ÑˆÐ° for finetuning data...\n",
      "\n",
      "Processing chat: EDB for finetuning data...\n",
      "\n",
      "Processing chat: Ed for finetuning data...\n",
      "\n",
      "Processing chat: Eugene Ð–ÐµÐ½ÐµÐº for finetuning data...\n",
      "\n",
      "Processing chat: Eugenii for finetuning data...\n",
      "\n",
      "Processing chat: Evgeny for finetuning data...\n",
      "\n",
      "Processing chat: FFCBÂ® banker for finetuning data...\n",
      "\n",
      "Processing chat: Giorgi Ð˜ÐŸ ÐŸÐ¾Ð¼Ð¾Ð³Ð°Ñ‚Ð¾Ñ€ for finetuning data...\n",
      "\n",
      "Processing chat: Gregory for finetuning data...\n",
      "\n",
      "Processing chat: Helenka Demidova for finetuning data...\n",
      "\n",
      "Processing chat: HuskyðŸ‡¬ðŸ‡ª for finetuning data...\n",
      "\n",
      "Processing chat: Igor for finetuning data...\n",
      "\n",
      "Processing chat: Ivan for finetuning data...\n",
      "\n",
      "Processing chat: Just Dude for finetuning data...\n",
      "\n",
      "Processing chat: Kalashnikova for finetuning data...\n",
      "\n",
      "Processing chat: Kermen for finetuning data...\n",
      "\n",
      "Processing chat: Kirill for finetuning data...\n",
      "\n",
      "Processing chat: Kris for finetuning data...\n",
      "\n",
      "Processing chat: Kseniya for finetuning data...\n",
      "\n",
      "Processing chat: Lake Bus Dev for finetuning data...\n",
      "\n",
      "Processing chat: Mantas for finetuning data...\n",
      "\n",
      "Processing chat: Mariya for finetuning data...\n",
      "\n",
      "Processing chat: Max for finetuning data...\n",
      "\n",
      "Processing chat: Mikhail for finetuning data...\n",
      "\n",
      "Processing chat: Nikita for finetuning data...\n",
      "\n",
      "Processing chat: Nikus for finetuning data...\n",
      "\n",
      "Processing chat: Ravestag Support for finetuning data...\n",
      "\n",
      "Processing chat: STANISLAV for finetuning data...\n",
      "\n",
      "Processing chat: Sauceory for finetuning data...\n",
      "\n",
      "Processing chat: Sberbank for finetuning data...\n",
      "\n",
      "Processing chat: Sergei Pauli for finetuning data...\n",
      "\n",
      "Processing chat: Sher1F Crypto for finetuning data...\n",
      "\n",
      "Processing chat: Stanislav for finetuning data...\n",
      "\n",
      "Processing chat: Stas for finetuning data...\n",
      "\n",
      "Processing chat: Surfskate_SMR for finetuning data...\n",
      "\n",
      "Processing chat: Swift for finetuning data...\n",
      "\n",
      "Processing chat: Tamara for finetuning data...\n",
      "\n",
      "Processing chat: Tatiana ÐŸÐ¾Ð´Ñ€ÑƒÐ³Ð° ÐœÐ°ÑˆÐ¸ for finetuning data...\n",
      "\n",
      "Processing chat: Telegram for finetuning data...\n",
      "\n",
      "Processing chat: _Awasaky_ for finetuning data...\n",
      "\n",
      "Processing chat: allpeg for finetuning data...\n",
      "\n",
      "Processing chat: chaga for finetuning data...\n",
      "\n",
      "Processing chat: mftidatascience for finetuning data...\n",
      "\n",
      "Processing chat: polina for finetuning data...\n",
      "\n",
      "Processing chat: tupik batumi hotel for finetuning data...\n",
      "\n",
      "Processing chat: valeriya Design for finetuning data...\n",
      "\n",
      "Processing chat: vanya_bffn for finetuning data...\n",
      "\n",
      "Processing chat: Â­Yury for finetuning data...\n",
      "\n",
      "Processing chat: Ä€dam for finetuning data...\n",
      "\n",
      "Processing chat: Ðwww ÐÑ€Ñ‚ÑƒÑ€ Ð¥Ð°ÐºÐ°Ñ‚Ð¾Ð½ for finetuning data...\n",
      "\n",
      "Processing chat: ÐÐ»ÐµÐºÑÐ°Ð½Ð´Ñ€Ð° for finetuning data...\n",
      "\n",
      "Processing chat: ÐÐ»ÐµÐºÑÐµÐ¹ for finetuning data...\n",
      "\n",
      "Processing chat: ÐÐ½Ð°ÑÑ‚Ð°ÑÐ¸Ñ for finetuning data...\n",
      "\n",
      "Processing chat: ÐÐ½Ð²Ð°Ñ€ for finetuning data...\n",
      "\n",
      "Processing chat: ÐÐ½Ð´Ñ€ÐµÐ¹ for finetuning data...\n",
      "\n",
      "Processing chat: ÐÐ½Ð½Ð° for finetuning data...\n",
      "\n",
      "Processing chat: ÐÑ€Ñ‚ÐµÐ¼ for finetuning data...\n",
      "\n",
      "Processing chat: Ð’ÐµÑ€Ð¾Ð½Ð¸ÐºÐ° for finetuning data...\n",
      "\n",
      "Processing chat: Ð’Ð¸ for finetuning data...\n",
      "\n",
      "Processing chat: Ð’Ð»Ð°Ð´Ð¸Ð¼Ð¸Ñ€ for finetuning data...\n",
      "\n",
      "Processing chat: Ð’Ð¾Ð²Ñ‡Ð¸Ðº ÐšÐ¾Ð»ÐµÑÐ½Ð¸ÐºÐ¾Ð² for finetuning data...\n",
      "\n",
      "Processing chat: Ð“Ð•Ð™ÐœÐ”Ð–Ð•Ðœ ÐžÐ¢ Â«ÐÐÐ§ÐÐ˜ Ð˜Ð“Ð Ð£Â» 24-27 ÐÐžÐ¯Ð‘Ð Ð¯ 2023 Ð³. for finetuning data...\n",
      "\n",
      "Processing chat: Ð“ÐµÐ¾Ñ€Ð³Ð¸Ð¹ for finetuning data...\n",
      "\n",
      "Processing chat: Ð”ÐµÐ½Ð¸Ñ for finetuning data...\n",
      "\n",
      "Processing chat: Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸Ð¹ for finetuning data...\n",
      "\n",
      "Processing chat: Ð•Ð³Ð¾Ñ€ for finetuning data...\n",
      "\n",
      "Processing chat: Ð•Ð»ÐµÐ½Ð° for finetuning data...\n",
      "\n",
      "Processing chat: Ð˜ÐŸ Ð“Ñ€ÑƒÐ·Ð¸Ñ ÐÐ½Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ð° ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€ for finetuning data...\n",
      "\n",
      "Processing chat: Ð˜Ð³Ð¾Ñ€ÑŒ for finetuning data...\n",
      "\n",
      "Processing chat: Ð˜Ð³Ñ€Ð° Ð² ÐºÐ°Ð»ÑŒÐ¼Ð°Ñ€Ð° Ð² ÐŸÑÑ‚Ð½Ð¸Ñ‡Ð½Ð¾Ð¼ Ð±Ð°Ñ€Ðµ for finetuning data...\n",
      "\n",
      "Processing chat: Ð˜Ñ€Ð° for finetuning data...\n",
      "\n",
      "Processing chat: ÐšÐ°Ð»Ð¸Ð½Ð¸Ð½ for finetuning data...\n",
      "\n",
      "Processing chat: ÐšÐ°Ñ‚Ñ for finetuning data...\n",
      "\n",
      "Processing chat: ÐšÐ°Ñ‚Ñ Ð§ÐµÑ€Ð½ÑƒÑ ÐÐ½Ñ‚Ð°Ð»Ð¸Ñ for finetuning data...\n",
      "\n",
      "Processing chat: ÐšÑÐµÐ½Ð¸Ñ for finetuning data...\n",
      "\n",
      "Processing chat: Ð›ÑƒÐ½Ð°Ñ€Ð½Ð°Ñ for finetuning data...\n",
      "\n",
      "Processing chat: ÐœÐ°ÑˆÐ° for finetuning data...\n",
      "\n",
      "Processing chat: ÐÐ°Ñ‚Ð°Ð»ÑŒÑ for finetuning data...\n",
      "\n",
      "Processing chat: ÐÐµ for finetuning data...\n",
      "\n",
      "Processing chat: ÐÐµÐ»Ð»Ð¸ (Ð›ÐµÐ¹ÐºÐ°) for finetuning data...\n",
      "\n",
      "Processing chat: ÐžÐºÑÐ°Ð½Ð° for finetuning data...\n",
      "\n",
      "Processing chat: ÐžÑÑ‚Ñ€Ð¾Ð²ÑÐºÐ°Ñ Chat for finetuning data...\n",
      "\n",
      "Processing chat: ÐŸÐ°Ð²ÐµÐ» for finetuning data...\n",
      "\n",
      "Processing chat: ÐŸÑ€Ð¸Ñ…Ð¾Ð¶ÑƒÐº for finetuning data...\n",
      "\n",
      "Processing chat: Ð Ð°Ð´Ð¼Ð¸Ð»Ð° for finetuning data...\n",
      "\n",
      "Processing chat: Ð¡Ð°ÑˆÐ° for finetuning data...\n",
      "\n",
      "Processing chat: Ð¡Ð°ÑˆÐ° ÐŸÑ€Ð¾Ñ…Ð¾Ñ€Ð¾Ð²Ð° for finetuning data...\n",
      "\n",
      "Processing chat: Ð¡Ð²ÐµÑ‚Ð»Ð°Ð½Ð° for finetuning data...\n",
      "\n",
      "Processing chat: Ð¡ÐµÐ¼Ñ‘Ð½ for finetuning data...\n",
      "\n",
      "Processing chat: Ð¡ÐµÑ€ÐµÐ³Ð° for finetuning data...\n",
      "\n",
      "Processing chat: Ð¢Ð°Ð½Ñ for finetuning data...\n",
      "\n",
      "Processing chat: á´˜á´€á´˜á´‡Ê€Ê™á´€á´„á´‹ á´¡Ê€Éªá´›á´‡Ê€ for finetuning data...\n",
      "\n",
      "Processing chat: ì—‘ì‹œ for finetuning data...\n",
      "\n",
      "Successfully created 19076 finetuning examples in 'ollama_finetune_dataset.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "def create_ollama_finetune_dataset(sorted_messages_df, source_id, output_finetune_file):\n",
    "    if sorted_messages_df.empty:\n",
    "        print(\"No messages provided to create finetuning dataset.\")\n",
    "        return\n",
    "\n",
    "    finetune_examples = []\n",
    "    current_instruction = None\n",
    "\n",
    "    for chat_id, chat_df in sorted_messages_df.groupby('chat_id'):\n",
    "        print(f\"\\nProcessing chat: {chat_id} for finetuning data...\")\n",
    "        current_instruction = \"\"\n",
    "\n",
    "        for index, message in chat_df.iterrows():\n",
    "            sender = message.get('from')\n",
    "            text = message.get('text')\n",
    "\n",
    "            if sender == source_id:\n",
    "                if len(current_instruction) > 0:\n",
    "                    finetune_examples.append({\n",
    "                        \"input\": current_instruction,\n",
    "                        \"output\": text\n",
    "                    })\n",
    "                    current_instruction = \"\"\n",
    "            else:\n",
    "                current_instruction += str(text)\n",
    "\n",
    "    try:\n",
    "        with open(output_finetune_file, 'w', encoding='utf-8') as f:\n",
    "            for example in finetune_examples:\n",
    "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "        print(f\"\\nSuccessfully created {len(finetune_examples)} finetuning examples in '{output_finetune_file}'.\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to output file '{output_finetune_file}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while writing '{output_finetune_file}': {e}\")\n",
    "\n",
    "create_ollama_finetune_dataset(sorted_df, \"Ilia Prokhorov (DemensDeum)\", \"ollama_finetune_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea654d79-5846-4fef-b235-b3c269c66fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13629836-ff57-4d5d-84e3-fa05b26cda1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.5: Fast Llama patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3080 Laptop GPU. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "target_model = \"unsloth/tinyllama-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = target_model,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea61cac-d5e4-430a-ac60-a138f65dc26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.5 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    \n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "083dcae5-9477-4887-9532-20d7903bba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "formatted_data = []\n",
    "\n",
    "with jsonlines.open(\"ollama_finetune_dataset.jsonl\", 'r') as reader:\n",
    "    for obj in reader:\n",
    "        output = f\"### Input: {obj['input']}\\n### Output: {json.dumps(obj['output'])}<|endoftext|>\"\n",
    "        formatted_data.append(output)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc63bc6-330b-41e8-8ce6-cfee1648a67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2653f82047444f8baa2bfbcdce36b451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/19076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 19,076 | Num Epochs = 1 | Total steps = 4,769\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 12,615,680 of 1,112,664,064 (1.13% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1945' max='4769' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1945/4769 1:04:21 < 1:33:32, 0.50 it/s, Epoch 0.41/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.042200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "default_train_params = {\n",
    "    \"max_seq_length\" : max_seq_length,\n",
    "    \"dataset_num_proc\" : 2,\n",
    "    \"per_device_train_batch_size\" : 2,\n",
    "    \"gradient_accumulation_steps\" : 4,\n",
    "    \"warmup_steps\" : 10,\n",
    "    \"num_train_epochs\" : 3,\n",
    "    \"learning_rate\" : 2e-4,\n",
    "    \"fp16\" : not torch.cuda.is_bf16_supported(),\n",
    "    \"bf16\" : torch.cuda.is_bf16_supported(),\n",
    "    \"logging_steps\" : 25,  \n",
    "    \"weight_decay\" : 0.01,\n",
    "    \"seed\" : 3407,\n",
    "    \"save_total_limit\" : 2,\n",
    "    \"dataloader_pin_memory\" : False\n",
    "}\n",
    "\n",
    "faster_train_params = {\n",
    "    \"max_seq_length\" : 512,\n",
    "    \"dataset_num_proc\" : 4,\n",
    "    \"per_device_train_batch_size\" : 4,\n",
    "    \"gradient_accumulation_steps\" : 1,\n",
    "    \"warmup_steps\" : 0,\n",
    "    \"num_train_epochs\" : 1,\n",
    "    \"learning_rate\" : 5e-4,\n",
    "    \"fp16\" : not torch.cuda.is_bf16_supported(),\n",
    "    \"bf16\" : torch.cuda.is_bf16_supported(),\n",
    "    \"logging_steps\" : 100,  \n",
    "    \"weight_decay\" : 0.0,\n",
    "    \"seed\" : 3407,\n",
    "    \"save_total_limit\" : 0,\n",
    "    \"dataloader_pin_memory\" : True\n",
    "}\n",
    "\n",
    "train_params = faster_train_params\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=train_params[\"max_seq_length\"],\n",
    "    dataset_num_proc=train_params[\"dataset_num_proc\"],\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=train_params[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps=train_params[\"gradient_accumulation_steps\"],  # Effective batch size = 8\n",
    "        warmup_steps=train_params[\"warmup_steps\"],\n",
    "        num_train_epochs=train_params[\"num_train_epochs\"],\n",
    "        learning_rate=train_params[\"learning_rate\"],\n",
    "        fp16=train_params[\"fp16\"],\n",
    "        bf16=train_params[\"bf16\"],\n",
    "        logging_steps=train_params[\"logging_steps\"],\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=train_params[\"weight_decay\"],\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=train_params[\"seed\"],\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=train_params[\"save_total_limit\"],\n",
    "        dataloader_pin_memory=train_params[\"dataloader_pin_memory\"],\n",
    "        report_to=\"none\", # Disable Weights & Biases logging\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5d454-258d-4636-a02d-98e48896d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9b065-e4d4-4334-b20a-50b7c1864df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
